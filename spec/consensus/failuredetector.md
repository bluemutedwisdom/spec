*** This is the beginning of an unfinished draft. Don't continue reading! ***

# Failure detector

A failure detector is a process that gets as input a header with some height *h*, connects to different Tendermint full nodes, requests the header of height *h* from them, and then cross-checks the headers and the input header.

There are two prime purposes:

1) to support fork accountability: In the case when more than 1/3 of the voting power is held by faulty validators, faulty nodes may generate two conflicting headers for the same height. The goal of the failure detector is to learn about the conflicting headers by probing different full nodes. Once a failure detector has two conflicting headers, these headers are evidence of misbehavior.

2) strengthen the lite client: If a lite client accepts a header *hd* (after performing skipping or sequential verification), it can use the failure detector to probe the system for conflicting header and increase the trust in *hd*. Instead of communicating with a single full node, communicating with several full nodes shall increase the probability to be aware of a fork in case there is one.



## Context of this document

### As service to the Lite client

The lite client specification is designed for the Tendermint failure model (1/3 assumption). It is safe and live under this assumption. If this assumption is violated, the lite client can be fooled to trust a header that was not generated by Tendermint consensus.

This specification, the failure detector, is a "second line of defense", in case the 1/3 assumption is violated. Its goal is to collect evidence. However,
for several reasons (in contrast to safety and liveness) we target probabilistic guarantees:

- it is impractical to probe all full nodes. That is, we will use random choice to decide which full node to probe next. As a result, there remains a probability that the full nodes that have a forged header or full nodes that have accepted forged headers (these nodes where led to disagree) are not probed (in time).

- even if we probe all full nodes, the adversary may hide forged headers from a full node until the failure detecter picks another full node.
**let's see what we can do about this...**

### As service to a Monitor (called Fisherman)

The failure detector takes as input a single header and probes the system for headers that are in conflict with *hd*. A natural extension is to use the failure detector within a monitor process that calls the failure detector on a sample (or all) headers (in parallel). If the sample is chosen at random, this adds another level of probabilistic reasoning.

Again, if conflicting headers are found, they are evidence that can be used for punishing processes.


### Evidence processing

How the generated evidence is used, will be discussed in another document. One solution would be that the failure detector tries to get the evidence accepted on the main chain, which may work in practical scenarios, e.g., if there was no attack on the main chain (agreement of the validators is never violated), but there was an attack on a light client.

 In theory, however,
given that the 1/3 assumption is necessarily violated to violate agreement, there are possible complications to submitting the evidence to the chain:

- evidence might be censored (faulty validators have enough votes to prevent deciding on a block that contains the evidence)
- the faulty processes block all progress. The chain comes to a halt.
- the faulty processes actually generated a fork and both branches make progress (with different correct processes participating in different branches). In this case none of the branches is "more correct" than the other one. We necessarily fall back to social consensus.

Due to these complications, in the worst case we may need to fall back to social consensus. Also for this, the evidence of the fault detector will be crucial to figure out who misbehaved/what went wrong. In this sense, this specification can be studied independently of the actual fork accountability and punishment scheme.


## Problem statement

- the failure detector maintains a set *pool* of tuples *(fn,hd,OK,proof)* consisting of a full node *fn*, a header *hd* provided by this full node, a boolean *OK* that indicates that the lite client algorithm established trust for this header, and the proof **to be done** returned by the lite client.

- We assume that the failure detector is initialized with a trusted header whose age is within the trusting period. That is, initially *pool = {(self,inithead,true,[])}*

- it runs the lite client algorithm (bisection) to trust newer headers, based on trusted old ones. If bisection reports an invalid header the "proof" that this header is invalid shall be added to a set *evidence*.

**this means that this evidence must be returned, e.g., by bisection. Bisection must always return the list of headers from inithead to newheader that was used to establish trust (or distrust) in newheader based on inithead**

- the failure detector requests headers from different full nodes (for heights it has already headers for in the pool), and checks them agains headers in the pool.

- In case there are two conflicting header for the same height, that is, there is a tuple *t = (_, th, true, _)* in the pool and a newly downloaded header *h* that are different but *th.height = h.height)*, then *t* and *h* shall be added to a set *evidence*.

** The goal is to find a (probabilistic) protocol that uses the lite client and probes full nodes directly in a way that ensures probabilistic guarantees of finding evidence in case it exists.

Questions

- Q1: if a full nodes never replies to requests can it be punished. Should each full node be required to at least serve a request every *x* minutes? Otherwise it is hard to motivate full nodes to respond, and thus to ensure any liveness guarantees. How can we reliably prove that a full node is not responding? How can we shield a correct full node from a faulty failure detector.

Possible response: It is not in the interest of faulty full nodes to talk to us. Also we cannot punish them. Hence we require that correct full nodes reliably talk to us. They have the incentive to do so, because we are helping them to understand whether their header is a good one.

- Q2: what is the most suitable way of modeling the system for probabilistic guarantees? Possible options from the literature:

  - Strong adversaries (that have a strategy in picking message delays to play against you, in combination with local coin tosses at the level of the failure detector)

  - Randomized schedulers: Roughly, a scheduler randomly decides which process does the next step and which messages are delivered to this process in this step.

  - perhaps just do a synchronous model: in step i the failure detector queries a full node, in step i+1 it does the computation based on the response (including deciding who to query next).

  - We have to understand how the blockchain grows while we do failure detection. How should the failure detector keep up?

-Q3: Are we limiting ourselves to the scenario of too many faults to ensure consensus agreement but not complete takeover, that is, 1/3 n <= f <= 2/3 n?






## Definitions

## Specification

## Solution
