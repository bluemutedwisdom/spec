*** This is the beginning of an unfinished draft. Don't continue reading! ***

# Failure detector

A failure detector is a process that connects to different Tendermint full nodes, requests headers from them, and then tries to verify and cross-check headers.

It uses the lite client algorithm to verify and trust headers, and then checks trusted headers against headers received from other full nodes.

Its prime purpose is to support fork accountability: In the case when more than 1/3 of the voting power is held by faulty validators, faulty nodes may generate two conflicting headers for the same height. The goal of the failure detector is to learn about the conflicting headers by probing different full nodes. Once a failure detector has two conflicting headers, these headers are evidence of misbehavior.


## Context of this document

The lite client specification is designed for the Tendermint failure model (1/3 assumption). It is safe and live under this assumption. If this assumption is violated, the lite client can be fooled to trust a header that was not generated by Tendermint consensus.

This specification, the failure detector, is a "second line of defense", in case the 1/3 assumption is violated. Its goal is to collect evidence. However,
for several reasons (in contrast to safety and liveness) we target probabilistic guarantees:

- it is impractical to probe all full nodes. That is, we will use random choice to decide which full node to probe next. As a result, there remains a probability that the full nodes that have a forged header or full nodes that have accepted forged headers (these nodes where led to disagree) are not probed (in time).

- even if we probe all full nodes, the adversary may hide forged headers from a full node until the failure detecter picks another full node.
**let's see what we can do about this...**

How this evidence is used, will be discussed in another document. One solution would be that the failure detector tries to get the evidence accepted on the main chain, which may work in practical scenarios, e.g., if there was no attack on the main chain (agreement of the validators is never violated), but there was an attack on a light client.

 In theory, however,
given that the 1/3 assumption is necessarily violated to violate agreement, there are possible complications to submitting the evidence to the chain:

- evidence might be censored (faulty validators have enough votes to prevent deciding on a block that contains the evidence)
- the faulty processes block all progress. The chain comes to a halt.
- the faulty processes actually generated a fork and both branches make progress (with different correct processes participating in different branches). In this case none of the branches is "more correct" than the other one. We necessarily fall back to social consensus.

Due to these complications, in the worst case we may need to fall back to social consensus. Also for this, the evidence of the fault detector will be crucial to figure out who misbehaved/what went wrong. In this sense, this specification can be studied independently of the actual fork accountability and punishment scheme.


## Problem statement

- the failure detector maintains a set *pool* of tuples *(fn,hd,OK,proof)* consisting of a full node *fn*, a header *hd* provided by this full node, a boolean *OK* that indicates that the lite client algorithm established trust for this header, and the proof **to be done** returned by the lite client.

- We assume that the failure detector is initialized with a trusted header whose age is within the trusting period. That is, initially *pool = {(self,inithead,true,[])}*

- it runs the lite client algorithm (bisection) to trust newer headers, based on trusted old ones. If bisection reports an invalid header the "proof" that this header is invalid shall be added to a set *evidence*.

**this means that this evidence must be returned, e.g., by bisection. Bisection must always return the list of headers from inithead to newheader that was used to establish trust (or distrust) in newheader based on inithead**

- the failure detector requests headers from different full nodes (for heights it has already headers for in the pool), and checks them agains headers in the pool.

- In case there are two conflicting header for the same height, that is, there is a tuple *t = (_, th, true, _)* in the pool and a newly downloaded header *h* that are different but *th.height = h.height)*, then *t* and *h* shall be added to a set *evidence*.

** The goal is to find a (probabilistic) protocol that uses the lite client and probes full nodes directly in a way that ensures probabilistic guarantees of finding evidence in case it exists.

Questions

- Q1: if a full nodes never replies to requests can it be punished. Should each full node be required to at least serve a request every *x* minutes? Otherwise it is hard to motivate full nodes to respond, and thus to ensure any liveness guarantees.

- Q2: what is the most suitable way of modeling the system for probabilistic guarantees? Possible options from the literature:

  - Strong adversaries (that have a strategy in picking message delays to play against you, in combination with local coin tosses at the level of the failure detector)

  - Randomized schedulers: Roughly, a scheduler randomly decides which process does the next step and which messages are delivered to this process in this step.

-Q3: Are we limiting ourselves to the scenario of too many faults to ensure consensus agreement but not complete takeover, that is, 1/3 n <= f <= 2/3 n?






## Definitions

## Specification

## Solution
